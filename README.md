# Data_Analytics-Week-1---2

# Content

# Module 1: The Basics of Data

# Introduction Chapter 1

Data analytics is a subject that transcends industry boundaries with skills like Machine Learning, Data Visualization and narrative crafting are no longer exclusive to data scientists and analysts.

It is vital to recognize that the domain of data analytics can be branched into 2 primary components:

****conceptual understanding of data(theoretical aspect)****

****hands-on manipulation of data(practical aspect)****

Both components can operate independently but the aim is to extract actionable insights and valuable information from data. Achieved through many ways, be it coding, low-code platforms or no-code solutions.

in this module: we will learn about:

1. The role of a Data Analyst and the scope of data analytics.

2. The process of data analytics

3. The techniques of data analytics.

4. Role of data governance



# Chapter 1: The Data Analyst


**What is Data Analytics?**


Virtually organizations collect large quantities of data about its customers, products, employees and service offerings.
Managers seek to analyze that data and harness the information it contains to improve the efficiency, effectiveness and profitability of their work.

The role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization. It involves several key responsibilities and skills.

1. **Data Collection and Preparation:**

   * Sourcing data from various channels, including databases, spreadsheets and external sources.
  
   * Cleaning and organization the data to ensure it is accurate, consistent and ready for analysis.

2. **Data Analysis:**

   * Employing statistical methods, machine learning techniques or other analytical tools to interpret data.

   * Identifying trends, patterns and correlations that might not be immediately obvious.

3. **Data Visualization and Storytelling:**
   
   * Creating visual representations of data such as graphs, charts and dashboards to make the complex information easily understandable.
  
   * Articulating findings in a compelling narrative to communicate the significance of data to stakeholders.
     
4. **Decision Support:**

   * Making recommendations based on data-driven insights to help guide business decisions.
  
   * Providing context around data including potential implications and future trends.

5. **Collaboration and Communication:**

   * Working closely with other departments to understand their needs and provide insights.
  
   * Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders.
  
6. **Continuous Learning and Adaptation:**

   * Keeping up-to-date with the latest industry trends, tools and technologies in data analysis.
  
   * Adapting to new types of data and analytical methods as the organization's needs evolve.
  
By fulfilling these roles, a data analyst helps an organization make informed decisions that can improve operational efficiencies, drive business strategies and create a competitive advantage. The goal is to have a hand in the organization's success by turning data into a valuable asset that informs and drives decision-making.

Data analyst is used to predict future sales or purchasing behaviours, to help and protect against fraud, analyze effectiveness of campaigns, boost customer acquisition and retention

5 steps a data analyst would take when approaching a project:

1. Define questions. take a problem to formulate a hypothesis.

2. Collect data from primary sources or internal, secondary or external sources.

3. Clean the data and prepare it for analysis.

4. Analyze the data: regression, cluster and time analysis.

5. interpret and share results


***World of Analytics***

Three major pillars that allow analytics programs to thrive

1. **Data**

In 2010, the total data stored was estimated at 5 exabytes.
Today, data is created daily at a rate exceeding the total information existing in 2010.
By 2025, the data generated is projected to reach 200 zettabytes (vastly exceeding earlier estimates).
The example of WhatsApp messages highlights this growth - daily messages would create a paper stack reaching over a million kilometers high annually.
The vast amount of data creates a huge opportunity for data analytics to extract valuable insights.

2. **Storage**

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/1e177b8c-7dbb-4e34-9a86-dfbc02c48d8a)


3. **Computing Power**

how Moore's Law and advancements in data storage have revolutionized data analytics.

Moore's Law: Predicted doubling of computing power every two years, making it cheaper and more accessible.
Increased Storage Availability: Data can now be stored cheaply, allowing for vast amounts to be retained for analysis.
Cloud Computing: Provides access to immense computing power on-demand and at low cost.
These factors combined create a perfect environment for data analytics to flourish.

***Careers in Analytics***

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/13fbaf5b-0fd0-4c80-a2f1-192baaffd5ce)


***Analytics Process***

Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data.

Data Acquisition >>> Cleaning & Manipulation >>> Analysis >>> Visualization >>> Reporting & Communication.

***Analytics Process is Iterative***

Analyzing data is similar to working on a puzzle, You dont have to follow the steps in a strict order. An analyst reviewing a visualization may notice unusual data points that do not seem to belong in the dataset, causing them to return to the data cleaning stage and rerun their analysis with newly cleaned dataset. Similarly to an analyst running an analysis might discover that their analysis would be enriched by adding another source of data, causing them to return to the data acquisiton stage.

This process is meant to help you understand the different activities that take place during a data analysis effort and the approximate order in which they typically occur.

***Analytics Techniques***

Analysts use a variety of techniques to come to conclusions from the data at their disposal. To understand the purpose of different types of analysis, they are grouped into categories based on the purpose of the analysis and nature of tool.

  * Descriptive Analytics

  * Predictive Analytics

  * Prescriptive Analytics


***Machine Learning, Artificial Intelligence and Deep Learning***

Machine Learning uses algorithms to discover knowledge in your datasets that you can then apply to help you make informed decisions about the future. 

Machine Learning commony adds value:

  * Segmented customers and determing the marketing messages that will appeal to different customer groups.

  * Discovering anomalies in system and application logs that may be indicative of cybersecurity incident.

  * Forecasting product sales based on market and environmental conditions.

  * Recommending the next movie that a customer might wish to watch based on their past activity and preference of similar customers.

  * Setting prices for hotel rooms far in advance based on forecasted demand.

Artificial Intelligence - includes any type of technique where you are attempting to get a computer system to imitate human behaviour. AI tries to mimic small portions of human behaviour and judgement.

Machine Learning - is a subset of AI techniques. ML techniques attempts to apply statistics to data problems in an effort to discover new knowledge. (ML techniques are AI techniques designed to learn)

Deep Learning - is a further subdivision of ML that uses quite compled techniques, known as neural networks, to discover knowledge in a particular way. It is a highly specialized subfield of ML that is most commonly used for image, video and sound analysis.

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/5a5e784b-7c0e-48fd-8961-c91c94b3d38d)

***Data Governance***

Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.

***Analytics Tools***

Software helps analysts work through each one of the phases of the analytics process. These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize and analyze data. Providing invaluable assistance in reporting and communicating results.

# Chapter Summary

Analytics allow businesses to access the untapped value locked within their data. Organizations recognize the potential value of this work but are still in early stages of developing their analytical programs. These programs (driven by availability of data, rapid decrease in cost of storage and maturation of cloud computing) promise to create significant opportunities for businesses and in turn for data professionals skilled in tools and techniques of analytics.

As analysts develop analytic work products, they generally move through a series of stages. The work begins with the acquisition of data from internal and external sources and conitnues with the cleaning and manipulation of that data. Once the data is in a suitable form, data professionals apply analytic techniques to draw up conclusions from their data, create visualizations to depict the story of their data and develop reports and dashboards to effectively communicate the results of their work to business leaders.

# Chapter 2: Understanding data

# Introduction Chapter 2

Knowledge of data includes understanding the various types of data that exist and the different options for storing that data in an enterprise environment.

In this chapter, you will: 

 * Understanding domain 1.0: data  concepts and environment.

 * Compare and contrast different data types.

 * Compare and contrast common data structures and file formats.


**Exploring Data Types**


To understand data types, it is best first to understand data elements. A data element is an attribute about a person, place, or thing containing data within a range of values. 

Data elements also describe characteristics of activities, including orders, transaction and events.

***Tabular Data***

This is data organized in a table, made up of rows and columns. It represents information on a single topic. Each column represents a uniquely named field within a table also called a variable about a single characteristic. The contents of each column contain values for the data element as defined by column header. 
Each row represents a record of a single instance of the table's topic. 

Spreadsheets including Microsoft Excel, Google Sheets and Apple Numbers are practical tools for representing tabular data. A relational database management system (database) extends the tabular model. Instead of having all data in a single table, a database organizes related data across multiple tables. The connection between tables is known as a relationship. Oracle, Microsoft SQL Server, MySQL and PostgreSQL are examples of database software.

***Structured Data Types***

Structured data is tabular in nature and organized into rows and columns.

**Character**

this data type limits data entry to only valid characters.Characters can include the alphabet that you might see on your keyboard, as well as numbers. Depending on your needs, multiple data types are available that can enforce character limits.

**Alphanumeric**

most widely used data type for storing character-based data. Alphanumeric is appropriate when a data element consists of both numbers and letters.  e.g an address of a place.

It is ideal for storing product stock-keeping units(SKUs).

***Character Sets***

When  considering an alphanumeric and text data type, you need to think about the character set you using to input and store data when using a database. Databases use character sets to map or encode data store it digitally. 

ASCII encoding standard is based on U.S. English alphabet. It accomodates both upper and lower case English alphabets and numbers, mathematical operators and symbols.

It is necessary to realize that individual characters may consume multiple bytes, impacting the length of a character string you can store in a character data type.

**Numeric**

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/bcceaeef-509d-4b52-9a4b-038d3e2c2739)

**Whole Numbers**

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/8554b102-8a67-4122-9c53-a9975b6a33aa)

**Rational Numbers**

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/e4a406bc-359f-45ce-a7be-9a94634d4983)

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/8bbcd6c3-8e0d-4029-a335-69b77df8c90c)

 
**Date and Time**

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/05af1c55-13ae-4cd0-80b3-c7e4f107262d)

**Currency**

It is essential to differentiate between data storage and data formatting. 

Data Storage - contains the actual value for a given data element. 

Data Formatting - takes a given data value and then formats it for display purposes, common when dealing with currency and data types.

**strong and Weak Typing**

Strong typing is whe technology rigidly enforces data types. 

Weak typing loosely enforces data types.

***Unstructured Data Types***

This is any type of data that does not fit neatly into the tabular model. e.g. digital images, audio recordings, video recordings and open-ended survey questions. 

To capture and analyze unstructured data, we make use of data types designed explicitly for that purpose.

**Binary**

They are one of the most common data types for storing unstructured data. 

It supports any type of digital file from Excel to spreadsheets. When considering which binary data type to use, file size tends to be the limiting factor. you need to select a data type that is large as the largest file you plan on storing.

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/2ed1cec7-18e5-4706-b1e0-bafbb15926ea)

*Images*

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/c2291d90-8db5-4682-b7f4-7f01addb8677)


![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/1c11ac9a-e630-4d77-9329-18de88116c11)

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/d3270609-48ee-43ac-8c0d-9a4e1d9c3ab4)

**Categories of Data**

*Quantitative vs Qualitative Data*

Quantitative data consists of numeric values. They are data elements whose values come from counting or measuring are quantitative. 
in a table, the Height and Weight column are quantitative. Quantitative data answers questions like "How many?" and "How much?"

Quantitative data consists of frequent text values. 
in a table, the Pet Name, Animal Type, Breed Name are qualitative. Qualitative data answers questions like "Why?" and "What?"

Data elements whose values describe characteristics, traits and attitudes are all qualitative.

*Discrete vs Continuous Data*

Numeric data comes in two different forms, discrete and continuous.

Discrete data represents things you can count like whole numbers(e.g. number of pets) or a value of specific increments (e.g. number of chickens bought in half-chicken portions).

Continuous data represents things you measure, often involving decimals (e.g. height, weight)

While both are numeric, discrete data has fixed values while continuous data can take on any value within a range.

Qualitative data is inherently discrete but quantitative can be either discrete or continuous depending on how it's used.


*Categorical data*

This one involves text data that falls into a limited set of predefined categories.

* Categorical data involves text values that belongs to specific groups (e.g. animal type: dog or cat)

* New data points are assigned to existing categories, but the categories themselves can be expanded if need (e.g. adding horses)

* Categorical data can be used for data validation. (e.g. an online appointment system can limit options to valid animal types (cat or dog) to prevent errors.)

*Dimensional Data*

   * Is an approach to arranging data to facilitate analysis. It organizes data into fact tables and dimension tables.

Fact tables store measurement data that is of interest to a business. A veterinary practice may want to answer some questions about appointments.  A table holding appointment data would be called a fact table. 

Dimensions are tables that contain data about the fact. For appointment data, the vet's office manager may want to understand who was at an appointment and if any procedures were performed. The Appointments table is the fact table. The vet, owners, procedures and pets are all dimensions that can answer question about appointment.

Dimensional data contains groupings of individual attributes about a given subject. (e.g. taken as a whole number, the pets dataset can be called "pets" dimensions

**Common Data Structures**

Data needs to be stored in a consistent, organized manner in order to facilitate analysis. When dealing with structured data, several concepts and standards inform how to organize data.

Analysts need to be able to perform their roles as efficiently as possible. It is common use to use multiple tools to analyze data. Improved integration and interoperability between tools make it easier for analysts to be productive. 

*Structured Data*

Tabular data is structured data with values stored in a consistent, defined manner, organized into columns and rows. Data is consistent when all entries in a column contain the same type of value.

This method of organization facilitates aggregation. (e.g. you can add each value in the Weight column to get the total weight of all animals.) 

It also makes summarization easy, since you can compute the average height for each animal. However structured data does not translate directly to data quality. 

*Unstructured Data*

Unstructured data is qualitative, describing characteristics of an event or an object. Images, phrases, audio or video recordings and descriptive texts are all examples of unstructured data. 

Its organizational and storage needs are different from structured data.

Machine Data is a common source of unstructured data. It has various sources including internet of Things devices, smartphones, tablets, pc and servers. They create digital footprints of their activity. 

A wide variety of technologies has emerged to facilitate the storage of unstructured data. These technologies  are similar to how a key in a tabular dataset identifies its associated values. The key is a unique identifier,  whereas the value is the unstructured data itself.

*Semi-structured Data*

it has some organization, unlike completely unstructured data, but it is not confined to a strict tabular format like spreadsheets. The need to make semi-structured data easier to work with has led to the emergence of semi-structured formatting options. They use separators or tags to provide some context around a data element.

***Common File Formats***

They facilitate data exchange and tool interoperability.

**Text Files**

These are plain texts often used for data due to their platform independence(readbility on different OS). They can also be created by machines e.g. log files

There are delimiters, they are structured data stored in text files, special characters(delimiters) separate individual data points(fields). Commas and Tabs are popular choices.

CSV & TSV (comma-seperated values & tab-separated values) are specific types of text files where commas or tabs act as delimiters. They hold structured data, mix of structured and unstructured data within a single field

**Fixed-width Files**

They have a fixed width for each column, meaning all data points in a specific column have the same length. The first row contains the column names, subsequent rows hold data entries. Each data entry must be padded with spaces or zeroes to maximum width allocated to its column.

**JavaScript Object Notation(JSON)**

JSON overview: JSON is a popular and open-standard file format that adds structure to plain text files. It's designed to be:

Human-readable: Easy for people to understand the data organization.

Machine-readable: Simple for programming languages to parse and process the data.

Lightweight: Doesn't add unnecessary complexity to the file size.

Data structure: JSON uses curly braces {} to enclose information about individual data entries. Within these braces, key-value pairs define the data elements and their corresponding values.

Machine processing: The text showcases how Python and R programming languages can easily read and process JSON data. It highlights how R can even generate summary statistics (e.g., number of dogs/cats, height/weight quartiles) from the JSON-formatted pet data.

Overall, JSON provides a clear and efficient way to store and exchange structured data that's both user and machine-friendly.

**Extensible Markup Language(XML)**

XML overview: XML is a markup language used to structure data in text files. Similar to JSON, it defines a way to organize information.

Key difference: Unlike JSON, XML uses tags (opening and closing tags) to denote data elements and their values. This makes XML more readable for humans but also leads to larger file sizes compared to JSON.

Impact on file size: While the size difference might be negligible for small files, it becomes significant for massive datasets (gigabytes or terabytes).

Historical context: In the past (around 1999), XML was a popular choice due to its role in web development techniques like Asynchronous JavaScript and XML (AJAX). AJAX allowed web pages to retrieve data from servers without needing a full page refresh, improving perceived speed.

Shift towards JSON: Today, JSON is gaining traction for asynchronous data exchange between web browsers and servers. This is likely due to its lighter weight and smaller file size compared to XML.

In essence, XML offers better readability for humans but comes with a file size penalty. JSON provides a more compact alternative that's becoming increasingly popular for data exchange.

**HyperText Markup Language(HTML)**

Purpose: HTML is a markup language used to create web pages displayed in web browsers. It's the foundation of how users interact with websites.

Structure: Similar to XML, HTML relies on tags to define the structure and content of a web page.

Evolution: HTML has become more advanced over time. Developers can now create dynamic web pages that adjust to screens, display videos, and include features like images.

Images in HTML tables: The passage demonstrates how images can be incorporated into HTML tables using the image tag (Figure 2.35). This allows displaying pictures of pets alongside their data in the table.

In summary, HTML provides a way to structure and format content displayed on web pages. It offers functionalities like tables and image embedding to present information in a user-friendly 

# Chapter 2 Summary

Data types: The choice of data type depends on the kind of data you're working with (dates, numbers, text, etc.) Choosing the right data type helps ensure data quality.

Structured data: This type of data is organized in a clear tabular format (rows and columns). Examples include dates, numbers, text entries, and currency values. Categorical data (types that fall into predefined categories) also falls under structured data.

Data formatting for structured data:
* CSV: A popular flat-file format suitable for storing structured data where data points are separated by commas (often used for exchanging data).
* JSON and XML: These are more complex formats that go beyond flat files and can handle additional metadata or intricate data structures.
* HTML: Understanding HTML is essential for working with data on the web because it's the standard language for structuring web pages. This knowledge can help analysts programmatically interact with web-based data sources.

In essence, the passage highlights the importance of choosing appropriate data types and formats based on the data you have. It emphasizes that structured data often benefits from a tabular structure and can be formatted using CSV, JSON, or XML. Finally, it stresses the importance of understanding HTML for interacting with data on the web.

1. Consider the values of what you will store before selecting data types

* Numeric data: This includes numbers with decimals. There are data types specifically designed to store them accurately.

* Whole numbers: For sequences of numbers without decimals (e.g., counting something), an integer data type is best suited.
  
* Text: Alphanumeric data types are ideal for storing text values.
  
* Dates and times: There are data types designed to handle both dates and times together, depending on your needs.
  
* Binary data (audio, video, images): A BLOB (Binary Large Object) data type is appropriate for storing these types of data.

2. Know that you can format data after storing it

  * Data types: These define how data is stored electronically (e.g., numeric data type for numbers).
     
  * Data formatting: This determines how data is presented to users (e.g., rounding numbers, applying currency symbols, adjusting date formats).

  
The key takeaway is that data storage (data type) and data presentation (formatting) are independent aspects. You can store data very precisely (many decimal places) but choose to display it in a more user-friendly way (rounded to two decimal places). The example highlights dates, where the same underlying data can be formatted differently based on cultural norms (e.g., DD/MM/YYYY vs. MM/DD/YYYY).

3. Consider the absolute limits of values that you will use before selecting data types

  * Discrete data: If the data can only have specific, separate values within a defined range (e.g., number of shoes owned: 0, 1, 2, etc.), choose a data type that supports discrete data.
  
     
  * Continuous data: If the data can theoretically have any value within a range (e.g., height: 1.5 meters, 1.51 meters, 1.512 meters, etc.), choose a data type that supports continuous data, especially if the exact range is unknown beforehand.

4. Explain the difference between structured and unstructured data

  * Highly structured, rectangular data: This sits at one extreme of the spectrum. It's very organized, resembling a table with rows and columns. Each column has a consistent data type (e.g., numbers, text), and each row represents information about a single entity (e.g., a customer in a customer database).

  * Unstructured data: This lies at the other end of the spectrum. It doesn't have a neat, tabular format. Analyzing similarities or differences within unstructured data requires more complex techniques compared to structured data.


In simpler terms, imagine data points arranged neatly in rows and columns like a spreadsheet on one side, and completely free-form text or images on the other side. Individual data elements can fall somewhere in between depending on their organization.

5. Understand differences in common file formats

  * Importance: These formats make data files readable by humans and allow different tools to work with the data (interoperability).
    
  * Delimited text files (CSV): This popular format uses commas (or tabs) to separate data points (fields) within a text file. CSV is a common choice for exchanging data.
    
  * XML and JSON: Designed for structured data, these formats offer more capabilities than CSV. They can include additional information about the data (metadata) and handle more intricate data structures.
    
  * JSON vs. XML: JSON is gaining preference due to its smaller file size (lower overhead) compared to XML.


In essence, the right file format depends on the data structure and complexity. CSV is a simple and common option, while XML and JSON provide more features for complex data. JSON is becoming increasingly popular due to its efficiency.


# Module 2: Data Preparation and Exploration

In this module, we will:

  * Understand how to explore and acquire data.
    
  * Learn about databases and the need to classify and store or structure data.

  * Compare various data manipulation techniques and how to manage data quality.

  * Explain the fundamentals of statistics and analysis techniques.

    
# Chapter 3: Databases and Data Acquisition


# Databases and Data Acquisition

In this chapter: 

  * Data concepts and environment

  * Identify basic concepts of data schemas and dimensions.

  * Understanding the domain of data mining.

  * Explain data acquisition concepts.

  * Explain common techniques for data manipulation and query optimization.

There are many different databases options to choose from when an organization needs to store data.

1. Relational

2. Nonrelational


*The Relational Model* 

The relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject. When creating an IT System, you need to consider all the entities required to make your system work.

The Entity Relationship Diagram  (ERD) is a visual artifact of the data modeling process. It shows the connection between related entities. A relationship is a connection between entities, the symbols adjacent to an entity describe the relationship.

Cardinality is the relationship between two entities, showing how many instances of one entity relate to instances of another entity. You specify the cardinality in an ERD with various line endings. 

The first component of the terminator indicates whether the relationship between the two entities is optional or required.

The second components indicates whether an entity instance in the first table is associated with a single entity instance in the related table or if an association can exist with multiple entity instances.

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/441935d7-c360-48c2-96a7-8eef95e87ee8)

When reading from an ER Diagram aloud from left to right, you say "An individual animal belongs to at least one and possibly many people". Reading from right to left, you say "A specific person has at least one and possibly many animals".

Unary relationship is when an entity has a connection with itself. e.g a single manager has multiple employees.

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/1505e022-25b5-4775-9c78-32288a85a767)


A binary relationship connects two entities. They are the most common and easy to explore whereas a unary and ternary are comparatively complex and rare.

A ternary relationship connects three entities.

Apart from being a helpful picture, the entity relationship diagram also serves as a relational database's blueprint. The ability to read ERDs helps you understand the structure of a relational database.

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/03a68e3c-3576-4f39-a303-c3ab72e694fe)

*Relational Databases*

Pieces of software that let you make an operational system out of an ERD. You start with a relational model and create a physical design. Relational entities correspond to databse tables and entuty attributes correspond to table columns. 

When creating a database table, the ordering of columns does not matter because you can specify the column order when retrieving the data from a table. When attributes becomes a column, you assign it a data type. 

![image](https://github.com/MisterWest11/Data_Analytics-Week-1---2/assets/152319557/94c311de-f1f6-4f5c-9507-80a7dea0a80b)

In the above figure, a new table was created. AnimalPerson is necessary because you need to resolve a many-to-many

*Chapter 4:* Data Quality

*Chapter 5:* Data Analytics and Statistics

# Module 3: Data Analysis and Reporting

*Chapter 6:* Data Analytics Tools

*Chapter 7:* Data Visualization with Reports and Dashboards

# Module 4: Data Governance

*Chapter 8:* Defining Data Governance


